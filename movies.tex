% Template for ICASSP-2013 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx,bm,setspace}
\usepackage{todonotes}
\setlength{\marginparwidth}{1.5cm}
\usepackage{lipsum}

% ADD THE FOLLOWING COUPLE LINES INTO YOUR PREAMBLE
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Pathological Speech Processing:\\State-of-the-Art, Current Challenges, and Future Directions}
%
% Single address.
% ---------------
\name{Rahul Gupta, Theodora Chaspari, Jangwon Kim, Naveen Kumar, Daniel Bone, Shrikanth Narayanan}
\address{Signal Analysis and Interpretation Lab,
        University of Southern California,  
  Los Angeles, CA, USA}  
\begin{document}
\ninept
%
\maketitle
%
\begin{abstract}
   Put abstract here 
 
\end{abstract}
%
\begin{keywords}
Pathological speech disorders, machine learning, signal processing 
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
Put intro here

\section{Background}
Put background here

\section{Dataset}
The data is taken from the LIRIS-ACCEDE(Annotated Creative Commons Emotional DatabaseE for affective video content analysis) database \cite{baveye2015liris} presented in the 2016 MediaEval Emotional Impact of Movies Task. It consists of two parts. The first part has 9800 video clips of length between 8 to 12 seconds taken from 160 movies. Each video has a single global valence and arousal rating in the range 1 to 5, annotated by 1517 annotators from 89 different countries using crowdsourcing. The second part has 30 short films of length varying from 3 to 28 minutes, each with a valence and arousal label in the range -1 to 1, for every second annotated by 10 participants.

\indent We use a variety of visual, speech and music features for training our model. The same features are extracted for both the video clips and the short films. The visual features are luminance, intensity and optical flow. The speech features are mel-frequency cepstral coefficients, voice probability, zero crossing rate, harmonic to noise ratio, fundamental frequency and logarithm of energy. We also include the first and second derivative, and the arithmic mean and standard deviation of the countour of the signal for every speech feature. Lastly we compute 12 semitones for our musical chroma features. We have used the OpenSMILE toolbox for feature extraction. A total of 9 statistics - mean, median, standard deviation, kurtosis, lower and upper quartile, minimum, maximum and range are computed for every feature. For the short films, we choose a 10 second window for calculating our statistics because it is the mean duration of the video clips. In total we have 156 visual, speech and musical features, each with nine statistics, making a total of 1404 features.

\section{Methodology}


\subsection{Baseline}
LR + ridge + NN

\subsection{Proposed method}

\subsubsection{Knowlege transfer}

\subsubsection{Gradient boosting}

\section{Results}

\subsection{Discussion}

\section{Conclusion}

\footnotesize{
\begin{spacing}{.85 }
%\vfill\pagebreak
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib2}
\bibliography{refs}
\end{spacing}
}
\end{document}
